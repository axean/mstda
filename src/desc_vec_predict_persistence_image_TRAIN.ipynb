{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3168c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gudhi as gd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import scipy.spatial as spatial\n",
    "import json\n",
    "from rich import print\n",
    "from random import choice, sample\n",
    "from tqdm.notebook import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from gc import collect\n",
    "from pickle import load\n",
    "\n",
    "with open(\"9701_cech_persistence_images_012_50x50.pickle\", mode = \"rb\") as data:\n",
    "    save_dict = load(data)\n",
    "    train_molecules = save_dict[\"train_molecules\"]\n",
    "    train_data = save_dict[\"train_data\"]\n",
    "    \n",
    "    test_molecules = save_dict[\"test_molecules\"]\n",
    "    test_data = save_dict[\"test_data\"]\n",
    "    \n",
    "%matplotlib inline\n",
    "del save_dict\n",
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f26218",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8ab1f",
   "metadata": {},
   "source": [
    "for ims in tqdm(train_data):\n",
    "    for im in ims:\n",
    "        max_px = im.max()\n",
    "        assert not (max_px < 0.0)\n",
    "        if max_px > 0.0:\n",
    "            im *= 1/max_px\n",
    "        \n",
    "for ims in tqdm(test_data):\n",
    "    for im in ims:\n",
    "        max_px = im.max()\n",
    "        assert not (max_px < 0.0)\n",
    "        if max_px > 0.0:\n",
    "            im *= 1/max_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a624eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[10,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347a34c",
   "metadata": {},
   "source": [
    "# Extract descriptors\n",
    "(this can be done from rdkit or from the descriptor .csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Drugbank_some_descriptors.csv\")\n",
    "cntr = 0\n",
    "for d in df:\n",
    "    cntr += 1\n",
    "print(cntr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e180ac",
   "metadata": {},
   "source": [
    "### Filter descriptors which are unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05265045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descriptors = list()\n",
    "\n",
    "# consider only descriptors which have few missing values and which are floating point\n",
    "for d in df:\n",
    "    if df[d].isna().sum() < 800 and df[d].dtype == np.float64:\n",
    "        descriptors.append(d)\n",
    "\n",
    "filter_descriptors = filter(lambda d : \"OEselma Descriptors\" not in d, descriptors)\n",
    "descriptors = list(set(descriptors) - set(filter_descriptors))\n",
    "num_descriptors = len(descriptors)\n",
    "print(descriptors)\n",
    "\n",
    "# select only smiles which have these descriptor values\n",
    "train_smiles = set(train_molecules)\n",
    "test_smiles = set(test_molecules)\n",
    "\n",
    "for i,d in enumerate(descriptors):\n",
    "    avail_rows = df[np.logical_not(df[d].isna())]\n",
    "    train_smiles = set(avail_rows[\"SMILES\"]).intersection(train_smiles)\n",
    "    test_smiles = set(avail_rows[\"SMILES\"]).intersection(test_smiles)\n",
    "\n",
    "train_idxs = np.asarray(sorted([ train_molecules.index(t) for t in train_smiles ]))\n",
    "test_idxs = np.asarray(sorted([ test_molecules.index(t) for t in test_smiles ]))\n",
    "\n",
    "train_molecules = np.asarray(train_molecules)\n",
    "train_molecules = train_molecules[train_idxs]\n",
    "train_data = train_data[train_idxs]\n",
    "\n",
    "test_molecules = np.asarray(test_molecules)\n",
    "test_molecules = test_molecules[test_idxs]\n",
    "test_data = test_data[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d54c",
   "metadata": {},
   "source": [
    "### Build numpy arrays of descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect()\n",
    "train_labels = np.empty((train_data.shape[0], num_descriptors))\n",
    "test_labels = np.empty((test_data.shape[0], num_descriptors))\n",
    "train_labels[:] = np.nan\n",
    "test_labels[:] = np.nan\n",
    "\n",
    "for i,mol in enumerate(tqdm(train_molecules)):\n",
    "    data_row = df.loc[df['SMILES'] == mol, descriptors ]\n",
    "    data_vec = [ r.to_list()[-1] for l,r in data_row.items() ]\n",
    "    train_labels[i,:] = np.asarray(data_vec)\n",
    "\n",
    "for i,mol in enumerate(tqdm(test_molecules)):\n",
    "    data_row = df.loc[df['SMILES'] == mol, descriptors ]\n",
    "    data_vec = [ r.to_list()[-1] for l,r in data_row.items() ]\n",
    "    test_labels[i,:] = np.asarray(data_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd107505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not np.isnan(train_labels).any(), not np.isnan(test_labels).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19245bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(2, suppress = True):\n",
    "    print(choice(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99dffe",
   "metadata": {},
   "source": [
    "# Setup CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Normalization(\n",
    "        input_shape = train_data.shape[1:],\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 3, # sliding window\n",
    "        data_format = \"channels_first\",\n",
    "        activation = \"relu\",\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 5,\n",
    "        activation = \"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(num_descriptors),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = .001),\n",
    "    loss = \"mean_absolute_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ab586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    verbose = 1,\n",
    "    epochs = 100,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab956c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot()\n",
    "    p1 = ax.plot(history.history['loss'], \"--\", color = \"black\")\n",
    "    p2 = ax.plot(history.history['val_loss'], color = \"black\")\n",
    "    ax.set_ylim([7, 25])\n",
    "    ax.set_xlabel('Epoch', fontsize = 20)\n",
    "    ax.set_ylabel('Mean Absolute Error', fontsize = 20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.legend([\"Training loss\", \"Validation loss\"], fontsize = 20)\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"(OEselma) ÄŒech complex persistence entropy, Regression CNN loss\", fontsize = 20)\n",
    "plt.rcParams.update({\n",
    "  \"text.usetex\": True,\n",
    "  \"font.family\": \"Times\"\n",
    "})\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(test_data).reshape((test_data.shape[0],num_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30a3bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_dict = dict()\n",
    "out_dict[\"Descriptor\"] = list()\n",
    "out_dict[\"_sigma\"] = list()\n",
    "out_dict[\"Average\"] = list()\n",
    "out_dict[\"Test Average\"] = test_labels.mean(axis = 0)\n",
    "out_dict[\"MAE\"] = np.abs(test_labels - predicted).mean(axis = 0)\n",
    "\n",
    "for i,d in enumerate(descriptors):\n",
    "    out_dict[\"Descriptor\"].append(d)\n",
    "    out_dict[\"Average\"].append(df[d].mean(skipna = True)) # population mean for this descriptor\n",
    "    out_dict[\"_sigma\"].append(df[d].std(skipna = True)) # population mean for this descriptor\n",
    "    \n",
    "out_dict[\"MAE/sigma\"] = out_dict[\"MAE\"]/out_dict[\"_sigma\"]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "out_df = pd.DataFrame(data = out_dict)\n",
    "print(out_dict[\"MAE\"].sum()/len(descriptors)) # THIS IS WHAT tf.keras.losses.MeanAbsoluteError() DOES\n",
    "out_df = out_df.sort_values(\"MAE/sigma\")\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20afa1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latex_dict = dict()\n",
    "latex_dict[\"Descriptors\"]                              = out_dict[\"Descriptor\"]\n",
    "latex_dict[\"$\\sigma$\"]                                 = out_dict[\"_sigma\"]\n",
    "latex_dict[\"$\\mu$\"]                                    = out_dict[\"Average\"]\n",
    "latex_dict[\"$\\overline{\\mathbf{f}_d}$\"]                = out_dict[\"Test Average\"]\n",
    "latex_dict[\"$\\overline{|\\mathbf{y}_d-\\mathbf{f}_d|}$\"] = out_dict[\"MAE\"] \n",
    "latex_dict[\"Score\"]                                    = out_dict[\"MAE/sigma\"]\n",
    "latex_df = pd.DataFrame(data = latex_dict)\n",
    "latex_df = latex_df.sort_values(\"Score\")\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49240f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_string = latex_df.to_latex(index = False, escape = False)\n",
    "latex_string = latex_string.replace(\"_\", \" \")\n",
    "latex_string = latex_string.replace(\"OEselma Descriptors;\", \"OEselma \")\n",
    "latex_string = latex_string.replace(\"&\\n\",\" & \")\n",
    "latex_string = latex_string.replace(\"\\n&\",\" & \")\n",
    "latex_string = latex_string.replace(\"& \\n\",\" & \")\n",
    "latex_string = latex_string.replace(\"MOE Descriptors;\", \"MOE \")\n",
    "\n",
    "lines = latex_string.split(\"\\n\")\n",
    "for i,l in enumerate(lines):\n",
    "    if i < 4 or i > len(lines)-4:\n",
    "        continue\n",
    "    \n",
    "    a = l.split(\"&\")\n",
    "    b = a[1:]\n",
    "    c = str.title(a[0].split(\";\")[0])\n",
    "    d = [ c ] + b\n",
    "    lines[i] = \"&\".join(d)\n",
    "latex_string = \"\\n\".join(lines)\n",
    "\n",
    "latex_string = latex_string.replace(\"&\", \" & \")\n",
    "\n",
    "for i in range(40):\n",
    "    latex_string = latex_string.replace(\"  \", \" \")\n",
    "\n",
    "\n",
    "print(latex_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 AZ1: Topological data Analysis",
   "language": "python",
   "name": "az1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
